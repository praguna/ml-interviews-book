#### 5.1.1 Vectors

_If some characters seem to be missing, it's because MathJax is not loaded correctly. Refreshing the page should fix it._

1. Dot product
    1. [E] Whatâ€™s the geometric interpretation of the dot product of two vectors? <br>
    Ans. Dot product of two vectors $$u$$ and $$v$$ represented as $$u.v$$ is the projection of $$u$$ on $$v$$. It is calculated (geometrically) as the product of euclidian magnitudes and the cosine of the angle between them. $$u.v = uv^T = ||u||_{2}||v||_{2}cos(\theta)$$.
    2. [E] Given a vector $$u$$, find vector $$v$$ of unit length such that the dot product of $$u$$ and $$v$$ is maximum. <br>
    Ans. As we saw the previous question, the dot product between $$u$$ and $$v$$ is maximum when the $$\theta = 0 \therefore v$$ has to be a unit vector in the direction of $$u$$, which is $$v = \frac{u}{||u||_2}$$

2. Outer product
    1. [E] Given two vectors $$a = [3, 2, 1]$$ and  $$b = [-1, 0, 1]$$. Calculate the outer product $$a^Tb$$?<br>
    Ans. <br> $$
	\begin{bmatrix}
		-3 & 0 & 3 \\
		-2 & 0 & 2 \\
		-1 & 0 & 1
	\end{bmatrix}
	$$
    1. [M] Give an example of how the outer product can be useful in ML. <br>
    Ans. The outer product is used in backpropagation propagation in a multilayer perceptron as follows :
    Consider an intermediate layer defined as : $$I.W = O$$, the derivative of O w.r.t W is <br> $$\frac{\partial O}{\partial W} = \frac{\partial{I.W}}{\partial W} = I$$, if the gradient of the output $$O$$ w.r.t to the final loss is $$G$$, then by chain rule we get the gradient of $$W$$ , $$W_g = I^T G$$ which is the <b>outer product</b>. (assuming $$I \text{ and } G$$ are a column vectors)
3. [E] What does it mean for two vectors to be linearly independent?<br>
Ans. Two vectors $$u$$ and $$v$$ are linearly independent if there is no linear combination satisfying a <b>zero vector</b> given by : $$x.u + y.v \neq \overrightarrow{0}$$, where $$x \text{ and } y$$ are scalars and not both are zero.
4. [M] Given two sets of vectors $$A = {a_1, a_2, a_3, ..., a_n}$$ and $$B = {b_1, b_2, b_3, ... , b_m}$$. How do you check that they share the same basis?<br>
Ans. $$A$$ and $$B$$ share the same basis if the basis of $$A$$ span all vectors in $$B$$. First we can solve for $$A.x = 0$$ after row reduction and prove that $$[B\text{ }S].x = 0$$ where S is the span of A. 
5. [M] Given $$n$$ vectors, each of $$d$$ dimensions. What is the dimension of their span?<br>
If the vectors are independent the span is $$d$$ dimensional, otherwise it is $$<d$$.

6. Norms and metrics
	1. [E] What's a norm? What is $$L_0, L_1, L_2, L_{norm}$$?<br>
    Ans. Norm refers to the length / magnitude of a vector.  It is a function that maps a vector to a real number, $$N(x) : x \implies R$$ . $$L_0$$ counts the number of non-zero elements , $$L_1$$ to $$L_n$$ can be expressed as : $$\sqrt[\leftroot{10} \uproot{2} n]{\sum b_{i}^n}$$ for a vector $$b = [b_1 \dots b_n] $$.

	1. [M] How do norm and metric differ? Given a norm, make a metric. Given a metric, can we make a norm? <br>
    Ans. Norm measures the magnitude of something whereas metric measure the distance between two things. Metric is a function of two variables and norm is of one. Metric can be converted into an <b>induced norm</b>. If a metric $$d(u, v)$$ is translation invariant, $$d(u + w, v + w) = d(u, v)$$ and scaling, $$d(u \alpha, v \alpha) = |\alpha| d(u, v)$$ converted to a norm as $$d(x , 0)$$.
